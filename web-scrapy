1.抓取并存储到text
html = urlopen('http://en.wikipedia.org/wiki/Python_(programming_language)')
bs = BeautifulSoup(html, 'html.parser')
content = bs.find('div', {'id': 'mw-content-text'}).get_text()

with open('wiki2.txt', 'w+', encoding='utf-8') as f: #打开文件，并指定编码格式
    for i in content:
        f.write(i)
2.读文件
Text
from urllib.request import urlopen
textPage = urlopen('http://x/x/test.txt')
print(textPage.read())
#print(str(textPage.read(), 'utf-8')) #指定编码格格式
-----------------------
html = urlopen('http://en.wikipedia.org/wiki/Python_(programming_language)')
bs = BeautifulSoup(html, 'html.parser')
content = bs.find('div', {'id':'mw-content-text'}).get_text()
content = bytes(content, 'UTF-8')
content = content.decode('UTF-8')

-----------------------
from urllib.request import urlopen
from io import StringIO
import csv

data = urlopen('http://pythonscraping.com/files/MontyPythonAlbums.csv').read().decode('ascii', 'ignore')
dataFile = StringIO(data) #StringIO像文件一样读写字符串buffer（内存文件）
csvReader = csv.reader(dataFile)

for row in csvReader:
    print(row)
-----以字典格式显示---
from urllib.request import urlopen
from io import StringIO
import csv

data = urlopen('http://pythonscraping.com/files/MontyPythonAlbums.csv').read().decode('ascii', 'ignore')
dataFile = StringIO(data)
dictReader = csv.DictReader(dataFile)
print(dictReader.fieldnames)  #csv第一行（标题行）
for row in dictReader:
    print(row)
3. 2分词
from urllib.request import urlopen
from bs4 import BeautifulSoup
from collections import Counter

import re
import string


def clenSentence(sentence):
    sentence = sentence.split(' ')
    sentence = [word.strip(string.punctuation+string.whitespace) for word in sentence]
    sentence = [word for word in sentence if len(word) > 1 or (word.lower() == 'a' or word.lower() == 'i')]
    return sentence


def cleanInput(content):
    content = content.upper()
    content = re.sub('\n', ' ', content)
    content = bytes(content, "UTF-8")
    content = content.decode("ascii", "ignore")
    sentences = content.split('. ')
    return [clenSentence(sentence) for sentence in sentences]

def getNgramsFromSentence(content, n):
    output = []
    for i in range(len(content)-n+1):
        output.append(content[i:i+n])
    return output


def getNgrams(content, n):
    content = cleanInput(content)
    ngrams = Counter()
    ngrams_list = []
    for sentence in content:
        newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, 2)]
        ngrams_list.extend(newNgrams)
        ngrams.update(newNgrams)
    return(ngrams)


content = str(urlopen('http://pythonscraping.com/files/inaugurationSpeech.txt').read(), 'utf-8')
# with open('speech_cleanI.txt', 'w') as f:
#     f.write(str(cleanInput(content)))
ngrams = getNgrams(content, 2)
print(ngrams)
